# ğŸ—ï¸ Kiáº¿n trÃºc há»‡ thá»‘ng ChatBot - Trá»£ lÃ½ há»c vá»¥

## ğŸ“Š Tá»•ng quan

Há»‡ thá»‘ng xá»­ lÃ½ vÃ  lÆ°u trá»¯ tÃ i liá»‡u há»c vá»¥ (PDF) â†’ TrÃ­ch xuáº¥t metadata â†’ LÆ°u vÃ o PostgreSQL + pgvector â†’ Semantic Search

---

## ğŸ—‚ï¸ Cáº¥u trÃºc thÆ° má»¥c Ä‘á» xuáº¥t

```
ChatBot/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw_pdfs/                      # PDFs gá»‘c (KHÃ”NG commit lÃªn Git)
â”‚   â”‚   â”œâ”€â”€ THONGBAO/                  # 34 files CTDT (ChÆ°Æ¡ng trÃ¬nh Ä‘Ã o táº¡o)
â”‚   â”‚   â”œâ”€â”€ QUY_DINH/                  # Quy Ä‘á»‹nh há»c phÃ­, rÃ¨n luyá»‡n
â”‚   â”‚   â””â”€â”€ QUY_CHE/                   # Quy cháº¿ Ä‘Ã o táº¡o
â”‚   â”‚
â”‚   â”œâ”€â”€ processed/                     # Káº¿t quáº£ xá»­ lÃ½
â”‚   â”‚   â”œâ”€â”€ json/                      # JSON structured data
â”‚   â”‚   â”‚   â”œâ”€â”€ THONGBAO/              # Theo tá»«ng nhÃ³m
â”‚   â”‚   â”‚   â”œâ”€â”€ QUY_DINH/
â”‚   â”‚   â”‚   â””â”€â”€ QUY_CHE/
â”‚   â”‚   â””â”€â”€ csv/                       # CSV Ä‘á»ƒ kiá»ƒm tra manual
â”‚   â”‚
â”‚   â””â”€â”€ logs/                          # Logs xá»­ lÃ½
â”‚       â””â”€â”€ batch_YYYYMMDD_HHMMSS.log
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ extractors/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ gemini_extractor.py        # Gemini API logic
â”‚   â”‚   â””â”€â”€ schemas.py                 # Pydantic schemas
â”‚   â”‚
â”‚   â”œâ”€â”€ storage/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ pgvector_storage.py        # PostgreSQL + pgvector
â”‚   â”‚   â””â”€â”€ embeddings.py              # Generate embeddings
â”‚   â”‚
â”‚   â”œâ”€â”€ search/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ semantic_search.py         # Vector similarity search
â”‚   â”‚
â”‚   â””â”€â”€ api/                           # FastAPI (Giai Ä‘oáº¡n 2)
â”‚       â”œâ”€â”€ __init__.py
â”‚       â””â”€â”€ main.py
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ batch_process.py               # Xá»­ lÃ½ hÃ ng loáº¡t PDFs
â”‚   â”œâ”€â”€ migrate_to_db.py               # Load JSON â†’ PostgreSQL
â”‚   â””â”€â”€ verify_data.py                 # Kiá»ƒm tra data quality
â”‚
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ test_extraction.py
â”‚
â”œâ”€â”€ main.py                            # Script xá»­ lÃ½ 1 file (hiá»‡n táº¡i)
â”œâ”€â”€ batch_processor.py                 # Batch processor (hiá»‡n táº¡i)
â”œâ”€â”€ pgvector_storage.py               # DB storage (hiá»‡n táº¡i)
â”œâ”€â”€ chatbot_storage.py                # Legacy storage
â”œâ”€â”€ docker-compose.yml                # PostgreSQL + pgvector
â”œâ”€â”€ init.sql                          # DB schema
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .env                              # API keys (KHÃ”NG commit)
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md
```

---

## ğŸ”„ Quy trÃ¬nh xá»­ lÃ½ dá»¯ liá»‡u

### **Giai Ä‘oáº¡n 1: TrÃ­ch xuáº¥t (Extraction Pipeline)**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PDF Files  â”‚
â”‚  (34 files) â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  batch_process.py        â”‚
â”‚  â”œâ”€ Scan folder          â”‚
â”‚  â”œâ”€ Upload to Gemini     â”‚
â”‚  â”œâ”€ Extract metadata     â”‚
â”‚  â””â”€ Save JSON + CSV      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  processed/json/         â”‚
â”‚  â”œâ”€ File1_output.json    â”‚
â”‚  â”œâ”€ File2_output.json    â”‚
â”‚  â””â”€ ...                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### **Giai Ä‘oáº¡n 2: LÆ°u trá»¯ (Storage Pipeline)**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  processed/json/*.json   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  migrate_to_db.py        â”‚
â”‚  â”œâ”€ Read JSON files      â”‚
â”‚  â”œâ”€ Generate embeddings  â”‚
â”‚  â””â”€ Insert to PostgreSQL â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PostgreSQL + pgvector   â”‚
â”‚  â”œâ”€ documents table      â”‚
â”‚  â”œâ”€ chunks table         â”‚
â”‚  â””â”€ vector index (HNSW) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### **Giai Ä‘oáº¡n 3: TÃ¬m kiáº¿m (Search Pipeline)**

```
User Query: "Há»c phÃ­ khÃ³a 2025?"
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  semantic_search.py      â”‚
â”‚  â”œâ”€ Embed query          â”‚
â”‚  â”œâ”€ Vector similarity    â”‚
â”‚  â””â”€ Rank results         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Top-K relevant chunks   â”‚
â”‚  + Full context          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ’¾ Chiáº¿n lÆ°á»£c lÆ°u trá»¯

### **1. Database Schema (PostgreSQL)**

```sql
-- Báº£ng documents (metadata cáº¥p tÃ i liá»‡u)
CREATE TABLE documents (
    id SERIAL PRIMARY KEY,
    doc_id VARCHAR(100) UNIQUE NOT NULL,
    file_name VARCHAR(255),
    doc_title TEXT,
    doc_type VARCHAR(50),          -- "ThÃ´ng bÃ¡o CTDT", "Quy Ä‘á»‹nh", etc.
    category VARCHAR(50),           -- "THONGBAO", "QUY_DINH", "QUY_CHE"
    issue_number VARCHAR(100),
    issuing_authority VARCHAR(255),
    issue_date DATE,
    effective_date VARCHAR(100),
    expiration_date DATE,
    major_topic VARCHAR(100),
    created_at TIMESTAMP DEFAULT NOW()
);

-- Báº£ng chunks (metadata cáº¥p Ä‘oáº¡n vÄƒn + vector)
CREATE TABLE chunks (
    id SERIAL PRIMARY KEY,
    chunk_id VARCHAR(150) UNIQUE NOT NULL,
    doc_id VARCHAR(100) REFERENCES documents(doc_id),
    page_number INTEGER,
    section_title TEXT,
    chunk_topic VARCHAR(255),
    content_type VARCHAR(100),     -- "Äáº¡i trÃ ", "CLCQ", etc.
    specific_target VARCHAR(255),
    applicable_cohort VARCHAR(100), -- "KhÃ³a 2024", "KhÃ³a 2025"
    value NUMERIC,
    unit VARCHAR(50),
    keywords TEXT[],
    chunk_text TEXT NOT NULL,
    embedding vector(768),         -- Gemini embedding
    created_at TIMESTAMP DEFAULT NOW()
);

-- Index cho vector search
CREATE INDEX ON chunks USING hnsw (embedding vector_cosine_ops);

-- Index cho filtering
CREATE INDEX idx_doc_category ON documents(category);
CREATE INDEX idx_chunk_cohort ON chunks(applicable_cohort);
CREATE INDEX idx_chunk_content_type ON chunks(content_type);
```

### **2. File Storage (JSON/CSV)**

**Lá»£i Ã­ch:**
- âœ… Backup dá»… dÃ ng
- âœ… Version control (Git LFS cho JSON)
- âœ… Dá»… debug vÃ  kiá»ƒm tra manual
- âœ… CÃ³ thá»ƒ re-import náº¿u DB bá»‹ lá»—i

**Tá»• chá»©c:**
```
processed/json/THONGBAO/
â”œâ”€â”€ CTDT_CNTT_K24_output.json
â”œâ”€â”€ CTDT_CNTT_K25_output.json
â””â”€â”€ ...

processed/csv/THONGBAO/
â”œâ”€â”€ CTDT_CNTT_K24_chunks.csv
â”œâ”€â”€ CTDT_CNTT_K25_chunks.csv
â””â”€â”€ ...
```

---

## ğŸš€ Káº¿ hoáº¡ch triá»ƒn khai

### **Phase 1: Xá»­ lÃ½ hÃ ng loáº¡t (Tuáº§n nÃ y)**

1. âœ… Cáº¥u trÃºc láº¡i thÆ° má»¥c
2. âœ… Cháº¡y batch_processor cho 34 files THONGBAO
3. âœ… Kiá»ƒm tra quality (sample 5-10 files)
4. âœ… Fix prompt náº¿u cáº§n

**Script chÃ­nh:**
```bash
python scripts/batch_process.py --input data/raw_pdfs/THONGBAO --output data/processed/json/THONGBAO
```

### **Phase 2: Migration vÃ o Database (Tuáº§n sau)**

1. âœ… Generate embeddings cho táº¥t cáº£ chunks
2. âœ… Load vÃ o PostgreSQL
3. âœ… Verify data integrity
4. âœ… Test semantic search

**Script chÃ­nh:**
```bash
python scripts/migrate_to_db.py --input data/processed/json/THONGBAO
```

### **Phase 3: Semantic Search API (2 tuáº§n ná»¯a)**

1. âœ… FastAPI endpoint
2. âœ… Query processing
3. âœ… Response formatting
4. âœ… Filtering by: cohort, ngÃ nh, loáº¡i CTDT

### **Phase 4: Frontend Integration (1 thÃ¡ng ná»¯a)**

1. âœ… Chat interface
2. âœ… Context-aware responses
3. âœ… Citation/source linking

---

## ğŸ”§ Tech Stack

| Layer | Technology | LÃ½ do |
|-------|-----------|-------|
| **Extraction** | Gemini 2.0 Flash | Structured output, há»— trá»£ PDF tá»‘t |
| **Validation** | Pydantic v2 | Type safety, schema validation |
| **Database** | PostgreSQL 16 | Reliable, máº¡nh máº½ |
| **Vector Store** | pgvector 0.8.1 | TÃ­ch há»£p sáºµn vá»›i PostgreSQL |
| **Embeddings** | Gemini Embedding | Miá»…n phÃ­, cháº¥t lÆ°á»£ng tá»‘t |
| **API** | FastAPI | Async, type hints, auto docs |
| **Container** | Docker Compose | Easy deployment |

---

## ğŸ“ˆ Æ¯á»›c tÃ­nh chi phÃ­ & Performance

### **Vá»›i 34 files THONGBAO:**
- **Gemini API calls**: ~34 uploads + 34 extractions = **FREE** (trong quota)
- **Storage**: ~34 JSON files Ã— 50KB = **~1.7MB**
- **Database**: ~34 documents + ~500 chunks Ã— 768D = **~5MB**
- **Processing time**: ~34 files Ã— 30s/file = **~17 phÃºt**

### **Scalability:**
- CÃ³ thá»ƒ xá»­ lÃ½ **1000+ files** mÃ  khÃ´ng váº¥n Ä‘á»
- pgvector HNSW index â†’ sub-second search vá»›i 10K+ chunks

---

## âœ… Action Items (NGAY BÃ‚Y GIá»œ)

1. **Cáº¥u trÃºc láº¡i thÆ° má»¥c** (5 phÃºt)
2. **Update batch_processor.py** Ä‘á»ƒ lÆ°u theo category (10 phÃºt)
3. **Cháº¡y batch processing cho THONGBAO** (20 phÃºt)
4. **Kiá»ƒm tra 5 files ngáº«u nhiÃªn** (15 phÃºt)

**Báº¡n muá»‘n tÃ´i lÃ m ngay bÆ°á»›c nÃ o?** ğŸš€
