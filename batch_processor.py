import os
import json
from pathlib import Path
from datetime import datetime
from pydantic import BaseModel, Field
from google import genai
from google.genai import types
from dotenv import load_dotenv

# Táº£i biáº¿n mÃ´i trÆ°á»ng
load_dotenv()

# Khá»Ÿi táº¡o client
api_key = os.getenv("GEMINI_API_KEY")
client = genai.Client(api_key=api_key)

# --- Schema Pydantic ---
class ThongBaoData(BaseModel):
    """Cáº¥u trÃºc dá»¯ liá»‡u cho má»™t ThÃ´ng BÃ¡o cáº§n lÆ°u vÃ o DB."""
    file_name: str = Field(description="TÃªn file gá»‘c.")
    tieu_de: str = Field(description="TiÃªu Ä‘á» Ä‘áº§y Ä‘á»§ cá»§a thÃ´ng bÃ¡o.")
    ngay_ban_hanh: str = Field(description="NgÃ y ban hÃ nh, Ä‘á»‹nh dáº¡ng YYYY-MM-DD.")
    don_vi_ban_hanh: str = Field(description="TÃªn Ä‘Æ¡n vá»‹ hoáº·c tá»• chá»©c ban hÃ nh thÃ´ng bÃ¡o.")
    trich_yeu: str = Field(description="TÃ³m táº¯t ná»™i dung chÃ­nh yáº¿u khÃ´ng quÃ¡ 30 tá»«.")
    noi_dung_quan_trong: list[str] = Field(description="CÃ¡c Ä‘iá»ƒm hoáº·c Ä‘iá»u khoáº£n quan trá»ng nháº¥t, dÆ°á»›i dáº¡ng danh sÃ¡ch.")
    noi_dung_thuan_text: str = Field(description="ToÃ n bá»™ ná»™i dung thÃ´ng bÃ¡o Ä‘Æ°á»£c chuyá»ƒn Ä‘á»•i sang dáº¡ng vÄƒn báº£n thuáº§n.")


# --- Xá»­ lÃ½ 1 file ---
def process_single_file(file_path: str) -> dict:
    """Xá»­ lÃ½ 1 file PDF/DOCX vÃ  tráº£ vá» dá»¯ liá»‡u Ä‘Ã£ trÃ­ch xuáº¥t."""
    
    file_name = os.path.basename(file_path)
    print(f"\n{'='*70}")
    print(f"ğŸ“„ Äang xá»­ lÃ½: {file_name}")
    print(f"{'='*70}")
    
    uploaded_file = None
    try:
        # Upload file
        print(f"ğŸ”„ Äang táº£i file lÃªn...")
        uploaded_file = client.files.upload(file=file_path)
        print(f"âœ… ÄÃ£ táº£i lÃªn: {uploaded_file.name}")
        
        # TrÃ­ch xuáº¥t dá»¯ liá»‡u
        prompt = (
            "Báº¡n lÃ  chuyÃªn gia phÃ¢n tÃ­ch tÃ i liá»‡u hÃ nh chÃ­nh Viá»‡t Nam.\n\n"
            "YÃŠU Cáº¦U:\n"
            "1. Äá»c ká»¹ toÃ n bá»™ tÃ i liá»‡u Ä‘Æ°á»£c cung cáº¥p\n"
            "2. TrÃ­ch xuáº¥t chÃ­nh xÃ¡c cÃ¡c trÆ°á»ng thÃ´ng tin theo schema\n"
            "3. Äáº£m báº£o ngÃ y ban hÃ nh Ä‘Ãºng Ä‘á»‹nh dáº¡ng YYYY-MM-DD\n"
            "4. Trich yáº¿u pháº£i sÃºc tÃ­ch, khÃ´ng quÃ¡ 30 tá»«\n"
            "5. Ná»™i dung quan trá»ng lÃ  cÃ¡c Ä‘iá»u khoáº£n, quy Ä‘á»‹nh chÃ­nh\n"
            "6. Chuyá»ƒn toÃ n bá»™ ná»™i dung sang vÄƒn báº£n thuáº§n, loáº¡i bá» kÃ½ tá»± Ä‘áº·c biá»‡t\n\n"
            "LÆ¯U Ã: Pháº£i chÃ­nh xÃ¡c 100%, khÃ´ng thÃªm tháº¯t hoáº·c bá»‹a Ä‘áº·t thÃ´ng tin."
        )
        
        print(f"ğŸ¤– Äang phÃ¢n tÃ­ch vá»›i Gemini AI...")
        response = client.models.generate_content(
            model='gemini-2.5-flash',
            contents=[prompt, uploaded_file],
            config=types.GenerateContentConfig(
                response_mime_type="application/json",
                response_schema=ThongBaoData,
                temperature=0.1,  # Äá»™ sÃ¡ng táº¡o tháº¥p = chÃ­nh xÃ¡c cao
            ),
        )
        
        # Parse JSON
        extracted_data = ThongBaoData.model_validate_json(response.text)
        data_dict = extracted_data.model_dump()
        
        # ThÃªm file_name
        data_dict['file_name'] = file_name
        
        # Táº¡o embedding
        print(f"ğŸ—ºï¸ Äang táº¡o vector embedding...")
        embed_response = client.models.embed_content(
            model='text-embedding-004',
            contents=data_dict['noi_dung_thuan_text'],
            config=types.EmbedContentConfig(task_type="RETRIEVAL_DOCUMENT")
        )
        data_dict['vector_data'] = embed_response.embeddings[0].values
        
        # Metadata
        data_dict['processed_at'] = datetime.now().isoformat()
        data_dict['model_used'] = 'gemini-2.5-flash'
        
        print(f"âœ… HoÃ n táº¥t xá»­ lÃ½: {file_name}")
        return data_dict
        
    except Exception as e:
        print(f"âŒ Lá»—i khi xá»­ lÃ½ {file_name}: {e}")
        return None
        
    finally:
        if uploaded_file:
            try:
                client.files.delete(name=uploaded_file.name)
                print(f"ğŸ—‘ï¸ ÄÃ£ xÃ³a file táº¡m")
            except:
                pass


# --- CÃCH 1: LÆ°u tá»«ng file riÃªng biá»‡t ---
def save_individual_files(results: list, output_dir: str = "extracted_data"):
    """LÆ°u má»—i document thÃ nh 1 file JSON riÃªng."""
    
    os.makedirs(output_dir, exist_ok=True)
    
    print(f"\n{'='*70}")
    print(f"ğŸ’¾ CÃCH 1: LÆ°u tá»«ng file riÃªng biá»‡t")
    print(f"{'='*70}")
    
    for data in results:
        if data is None:
            continue
            
        # Táº¡o tÃªn file output tá»« file gá»‘c
        original_name = Path(data['file_name']).stem
        output_file = os.path.join(output_dir, f"{original_name}_data.json")
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… ÄÃ£ lÆ°u: {output_file}")
    
    print(f"\nğŸ“ Tá»•ng cá»™ng: {len([d for d in results if d])} files")


# --- CÃCH 2: LÆ°u táº¥t cáº£ vÃ o 1 file JSON duy nháº¥t ---
def save_single_json(results: list, output_file: str = "all_documents.json"):
    """LÆ°u táº¥t cáº£ documents vÃ o 1 file JSON."""
    
    print(f"\n{'='*70}")
    print(f"ğŸ’¾ CÃCH 2: LÆ°u táº¥t cáº£ vÃ o 1 file JSON")
    print(f"{'='*70}")
    
    # Lá»c bá» None
    valid_results = [r for r in results if r is not None]
    
    output_data = {
        "metadata": {
            "total_documents": len(valid_results),
            "processed_at": datetime.now().isoformat(),
            "model_used": "gemini-2.5-flash"
        },
        "documents": valid_results
    }
    
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(output_data, f, ensure_ascii=False, indent=2)
    
    print(f"âœ… ÄÃ£ lÆ°u: {output_file}")
    print(f"ğŸ“Š Tá»•ng sá»‘ documents: {len(valid_results)}")
    file_size = os.path.getsize(output_file) / 1024  # KB
    print(f"ğŸ“¦ KÃ­ch thÆ°á»›c file: {file_size:.2f} KB")


# --- CÃCH 3: LÆ°u theo cáº¥u trÃºc phÃ¢n cáº¥p (theo thá»i gian/loáº¡i) ---
def save_hierarchical(results: list, base_dir: str = "data_hierarchy"):
    """LÆ°u theo cáº¥u trÃºc thÆ° má»¥c phÃ¢n cáº¥p (theo nÄƒm/thÃ¡ng/loáº¡i)."""
    
    print(f"\n{'='*70}")
    print(f"ğŸ’¾ CÃCH 3: LÆ°u theo cáº¥u trÃºc phÃ¢n cáº¥p")
    print(f"{'='*70}")
    
    for data in results:
        if data is None:
            continue
        
        try:
            # Parse ngÃ y ban hÃ nh
            ngay_ban_hanh = data.get('ngay_ban_hanh', '2025-01-01')
            year = ngay_ban_hanh.split('-')[0]
            month = ngay_ban_hanh.split('-')[1]
            
            # XÃ¡c Ä‘á»‹nh loáº¡i document (dá»±a vÃ o tá»« khÃ³a trong tiÃªu Ä‘á»)
            tieu_de = data.get('tieu_de', '').lower()
            if 'há»c phÃ­' in tieu_de or 'hoc phi' in tieu_de:
                doc_type = 'hoc_phi'
            elif 'thÃ´ng bÃ¡o' in tieu_de or 'thong bao' in tieu_de:
                doc_type = 'thong_bao'
            elif 'quyáº¿t Ä‘á»‹nh' in tieu_de or 'quyet dinh' in tieu_de:
                doc_type = 'quyet_dinh'
            else:
                doc_type = 'khac'
            
            # Táº¡o cáº¥u trÃºc thÆ° má»¥c: base_dir/nÄƒm/thÃ¡ng/loáº¡i/
            dir_path = os.path.join(base_dir, year, month, doc_type)
            os.makedirs(dir_path, exist_ok=True)
            
            # Táº¡o tÃªn file
            original_name = Path(data['file_name']).stem
            output_file = os.path.join(dir_path, f"{original_name}.json")
            
            # LÆ°u file
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
            
            print(f"âœ… ÄÃ£ lÆ°u: {output_file}")
            
        except Exception as e:
            print(f"âš ï¸ Lá»—i khi lÆ°u {data.get('file_name', 'unknown')}: {e}")
    
    print(f"\nğŸ“‚ Cáº¥u trÃºc thÆ° má»¥c Ä‘Ã£ táº¡o:")
    print(f"   {base_dir}/")
    print(f"   â”œâ”€â”€ 2025/")
    print(f"   â”‚   â”œâ”€â”€ 01/")
    print(f"   â”‚   â”‚   â”œâ”€â”€ hoc_phi/")
    print(f"   â”‚   â”‚   â”œâ”€â”€ thong_bao/")
    print(f"   â”‚   â”‚   â””â”€â”€ quyet_dinh/")


# --- CÃCH 4: LÆ°u vÃ o SQLite Database ---
def save_to_sqlite(results: list, db_file: str = "documents.db"):
    """LÆ°u vÃ o SQLite database."""
    import sqlite3
    
    print(f"\n{'='*70}")
    print(f"ğŸ’¾ CÃCH 4: LÆ°u vÃ o SQLite Database")
    print(f"{'='*70}")
    
    # Táº¡o database vÃ  báº£ng
    conn = sqlite3.connect(db_file)
    cursor = conn.cursor()
    
    cursor.execute('''
        CREATE TABLE IF NOT EXISTS thong_bao (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            file_name TEXT,
            tieu_de TEXT,
            ngay_ban_hanh TEXT,
            don_vi_ban_hanh TEXT,
            trich_yeu TEXT,
            noi_dung_quan_trong TEXT,
            noi_dung_thuan_text TEXT,
            vector_data TEXT,
            processed_at TEXT,
            model_used TEXT
        )
    ''')
    
    # Insert dá»¯ liá»‡u
    count = 0
    for data in results:
        if data is None:
            continue
        
        cursor.execute('''
            INSERT INTO thong_bao (
                file_name, tieu_de, ngay_ban_hanh, don_vi_ban_hanh,
                trich_yeu, noi_dung_quan_trong, noi_dung_thuan_text,
                vector_data, processed_at, model_used
            ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
        ''', (
            data.get('file_name'),
            data.get('tieu_de'),
            data.get('ngay_ban_hanh'),
            data.get('don_vi_ban_hanh'),
            data.get('trich_yeu'),
            json.dumps(data.get('noi_dung_quan_trong', []), ensure_ascii=False),
            data.get('noi_dung_thuan_text'),
            json.dumps(data.get('vector_data', []), ensure_ascii=False),
            data.get('processed_at'),
            data.get('model_used')
        ))
        count += 1
    
    conn.commit()
    conn.close()
    
    print(f"âœ… ÄÃ£ lÆ°u {count} documents vÃ o database: {db_file}")
    print(f"ğŸ“Š Báº£ng: thong_bao")


# --- Main Function ---
def batch_process_documents(input_dir: str = ".", file_pattern: str = "*.pdf"):
    """Xá»­ lÃ½ hÃ ng loáº¡t cÃ¡c file PDF/DOCX."""
    
    print("="*70)
    print("ğŸš€ Báº®T Äáº¦U Xá»¬ LÃ HÃ€NG LOáº T TÃ€I LIá»†U")
    print("="*70)
    
    # TÃ¬m táº¥t cáº£ file PDF
    pdf_files = list(Path(input_dir).glob(file_pattern))
    
    if not pdf_files:
        print(f"âš ï¸ KhÃ´ng tÃ¬m tháº¥y file {file_pattern} trong thÆ° má»¥c {input_dir}")
        return
    
    print(f"\nğŸ“ TÃ¬m tháº¥y {len(pdf_files)} files:")
    for i, f in enumerate(pdf_files, 1):
        print(f"   {i}. {f.name}")
    
    # Xá»­ lÃ½ tá»«ng file
    results = []
    for file_path in pdf_files:
        result = process_single_file(str(file_path))
        results.append(result)
    
    # Thá»‘ng kÃª
    successful = len([r for r in results if r is not None])
    failed = len([r for r in results if r is None])
    
    print(f"\n{'='*70}")
    print(f"ğŸ“Š THá»NG KÃŠ")
    print(f"{'='*70}")
    print(f"âœ… ThÃ nh cÃ´ng: {successful}/{len(results)}")
    print(f"âŒ Tháº¥t báº¡i: {failed}/{len(results)}")
    
    if successful == 0:
        print("\nâš ï¸ KhÃ´ng cÃ³ dá»¯ liá»‡u Ä‘á»ƒ lÆ°u!")
        return
    
    # LÆ°u dá»¯ liá»‡u
    print(f"\n{'='*70}")
    print(f"ğŸ’¾ LÆ¯U TRá»® Dá»® LIá»†U")
    print(f"{'='*70}")
    
    # LÆ¯U VÃ€O SQLite (CHÃNH)
    save_to_sqlite(results, "output/documents.db")
    
    # BACKUP VÃ€O JSON
    save_single_json(results, "output/all_documents.json")
    
    print(f"\n{'='*70}")
    print(f"ğŸ‰ HOÃ€N Táº¤T!")
    print(f"{'='*70}")
    print(f"\nğŸ“‚ Dá»¯ liá»‡u Ä‘Ã£ Ä‘Æ°á»£c lÆ°u:")
    print(f"   ğŸ—„ï¸ SQLite (chÃ­nh): output/documents.db")
    print(f"   ğŸ’¾ JSON (backup): output/all_documents.json")
    print(f"\nğŸ’¡ Sá»¬ Dá»¤NG:")
    print(f"   python chatbot_storage.py        # Demo query & search")
    print(f"   python demo_storage_query.py     # Xem chi tiáº¿t query")


if __name__ == "__main__":
    # Xá»­ lÃ½ táº¥t cáº£ file PDF trong thÆ° má»¥c hiá»‡n táº¡i
    batch_process_documents(input_dir=".", file_pattern="*.pdf")
